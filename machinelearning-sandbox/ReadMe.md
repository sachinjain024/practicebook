# Machine Learning Notes
------------------------

## Bias-Variance TradeOff
- Bias and Variance are two terms referred for telling the degree of underfit and overfit of a model
- Bagging algorithms control only variance
- Boosting algorithms controls both bias as well as variance

## Decision Trees
- Can be used for regression problem (Remember kaggle house prediction problem)
- We define certain boundaries in the training data set and kinda form clusters 
- Each cluster/group gets a score ~ average of the scrore of each indivual point
- Now for test data point, we check the boundaries and based on that we reach some group defined above
- We don't need to convert categorical inputs to numerical inputs for decision trees

## Random Forests
- We create N decision trees and run the test data point on each decicion tree
- Then take the average of the score given by each decision tree

## Gradient Boosting
- Boosting is a method of parameter tuning of the model while controlling bias and variance


clock sync deviation margin
reverse the lines in 3rd box
split provisioning info into requirements and allocations
tables are geo local
status in storage services and repliva
status in bootstrap info
flatten table definitions and column name should be config
status
